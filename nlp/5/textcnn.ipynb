{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "class TextCNN(object):\n",
    "    def __init__(self,\n",
    "                 num_classes,\n",
    "                 seq_length,\n",
    "                 vocab_size,\n",
    "                 embedding_dim,\n",
    "                 learning_rate,\n",
    "                 learning_decay_rate,\n",
    "                 learning_decay_steps,\n",
    "                 epoch,\n",
    "                 filter_sizes,\n",
    "                 num_filters,\n",
    "                 dropout_keep_prob,\n",
    "                 l2_lambda\n",
    "                 ):\n",
    "        self.num_classes = num_classes\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.learning_decay_rate = learning_decay_rate\n",
    "        self.learning_decay_steps = learning_decay_steps\n",
    "        self.epoch = epoch\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.num_filters = num_filters\n",
    "        self.dropout_keep_prob = dropout_keep_prob\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, self.seq_length], name='input_x')\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, self.num_classes], name='input_y')\n",
    "        self.l2_loss = tf.constant(0.0)\n",
    "        self.model()\n",
    "\n",
    "    def model(self):\n",
    "        # embedding层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            self.embedding= tf.Variable(tf.random_uniform([self.vocab_size, self.embedding_dim], -1.0, 1.0),\n",
    "                                        name=\"embedding\")\n",
    "            self.embedding_inputs = tf.nn.embedding_lookup(self.embedding,self.input_x)\n",
    "            self.embedding_inputs = tf.expand_dims(self.embedding_inputs,-1)\n",
    "\n",
    "        # 卷积层 + 池化层\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(self.filter_sizes):\n",
    "            with tf.name_scope(\"conv_{0}\".format(filter_size)):\n",
    "                filter_shape = [filter_size, self.embedding_dim, 1, self.num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedding_inputs,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\"\n",
    "                )\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, self.seq_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\"\n",
    "                )\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # 将每种尺寸的卷积核得到的特征向量进行拼接\n",
    "        num_filters_total = self.num_filters * len(self.filter_sizes)\n",
    "        h_pool = tf.concat(pooled_outputs, 3)\n",
    "        h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # 对最终得到的句子向量进行dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            h_drop = tf.nn.dropout(h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # 全连接层\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\"W\",shape=[num_filters_total, self.num_classes],\n",
    "                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[self.num_classes]), name=\"b\")\n",
    "            self.l2_loss += tf.nn.l2_loss(W)\n",
    "            self.l2_loss += tf.nn.l2_loss(b)\n",
    "            self.logits = tf.nn.xw_plus_b(h_drop, W, b, name=\"scores\")\n",
    "            self.pred = tf.argmax(self.logits, 1, name=\"predictions\")\n",
    "\n",
    "        # 损失函数\n",
    "        self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.input_y) + self.l2_lambda*self.l2_loss\n",
    "\n",
    "        # 优化函数\n",
    "        self.global_step = tf.train.get_or_create_global_step()\n",
    "        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step,\n",
    "                                                   self.learning_decay_steps, self.learning_decay_rate,\n",
    "                                                   staircase=True)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        self.optim = slim.learning.create_train_op(total_loss=self.loss, optimizer=optimizer, update_ops=update_ops)\n",
    "\n",
    "        # 准确率\n",
    "        self.acc = tf.metrics.accuracy(predictions=self.logits, labels=self.input_y)\n",
    "\n",
    "    def fit(self,train_x,train_y,val_x,val_y,batch_size):\n",
    "        # 创建模型保存路径\n",
    "        if not os.path.exists('./saves/textcnn'): os.makedirs('./saves/textcnn')\n",
    "        if not os.path.exists('./train_logs/textcnn'): os.makedirs('./train_logs/textcnn')\n",
    "\n",
    "        # 开始训练\n",
    "        train_steps = 0\n",
    "        best_val_acc = 0\n",
    "        # summary\n",
    "        tf.summary.scalar('val_loss', self.loss)\n",
    "        tf.summary.scalar('val_acc', self.acc)\n",
    "        merged = tf.summary.merge_all()\n",
    "\n",
    "        # 初始化变量\n",
    "        sess = tf.Session()\n",
    "        writer = tf.summary.FileWriter('./train_logs/textcnn', sess.graph)\n",
    "        saver = tf.train.Saver(max_to_keep=10)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for i in range(self.epoch):\n",
    "            batch_train = self.batch_iter(train_x, train_y, batch_size)\n",
    "            for batch_x,batch_y in batch_train:\n",
    "                train_steps += 1\n",
    "                feed_dict = {self.input_x:batch_x,self.input_y:batch_y}\n",
    "                _, train_loss, train_acc = sess.run([self.optim,self.loss,self.acc],feed_dict=feed_dict)\n",
    "\n",
    "                if train_steps % 1000 == 0:\n",
    "                    feed_dict = {self.input_x:val_x,self.input_y:val_y}\n",
    "                    val_loss,val_acc = sess.run([self.loss,self.acc],feed_dict=feed_dict)\n",
    "\n",
    "                    summary = sess.run(merged,feed_dict=feed_dict)\n",
    "                    writer.add_summary(summary, global_step=train_steps)\n",
    "\n",
    "                    if val_acc>=best_val_acc:\n",
    "                        best_val_acc = val_acc\n",
    "                        saver.save(sess, \"./saves/textcnn/\", global_step=train_steps)\n",
    "\n",
    "                    msg = 'epoch:%d/%d,train_steps:%d,train_loss:%.4f,train_acc:%.4f,val_loss:%.4f,val_acc:%.4f'\n",
    "                    print(msg % (i,self.epoch,train_steps,train_loss,train_acc,val_loss,val_acc))\n",
    "\n",
    "        sess.close()\n",
    "\n",
    "    def batch_iter(self, x, y, batch_size=32, shuffle=True):\n",
    "        \"\"\"\n",
    "        生成batch数据\n",
    "        :param x: 训练集特征变量\n",
    "        :param y: 训练集标签\n",
    "        :param batch_size: 每个batch的大小\n",
    "        :param shuffle: 是否在每个epoch时打乱数据\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        data_len = len(x)\n",
    "        num_batch = int((data_len - 1) / batch_size) + 1\n",
    "\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_len))\n",
    "            x_shuffle = x[shuffle_indices]\n",
    "            y_shuffle = y[shuffle_indices]\n",
    "        else:\n",
    "            x_shuffle = x\n",
    "            y_shuffle = y\n",
    "        for i in range(num_batch):\n",
    "            start_index = i * batch_size\n",
    "            end_index = min((i + 1) * batch_size, data_len)\n",
    "            yield x_shuffle[start_index:end_index], y_shuffle[start_index:end_index]\n",
    "\n",
    "    def predict(self,x):\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        ckpt = tf.train.get_checkpoint_state('./saves/textcnn/')\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "        feed_dict = {self.input_x: x}\n",
    "        logits = sess.run(self.logits, feed_dict=feed_dict)\n",
    "        y_pred = np.argmax(logits, 1)\n",
    "        return y_pred\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}